{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FINAL PROJECT REPLICATION:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy import linalg, optimize\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. DATA LOADING\n",
    "\n",
    "def get_real_data():\n",
    "    tickers = [\n",
    "        \"GOOGL\", \"AAPL\", \"MSFT\", \"MU\", \"INTC\", \"CSCO\", \"BIDU\", \"ADBE\", \"TXN\", \"AVGO\",\n",
    "        \"QCOM\", \"AMT\", \"HPQ\", \"IBM\", \"ORCL\", \"NTAP\", \"GLW\", \"AMAT\", \"AMZN\", \"LOGI\"\n",
    "    ]\n",
    "    print(\"Downloading data for 20 Tech Stocks (2015-2017)...\")\n",
    "    data = yf.download(tickers, start=\"2015-06-01\", end=\"2017-06-01\")[\"Close\"]\n",
    "    \n",
    "    # Log Returns\n",
    "    returns = np.log(data / data.shift(1)).dropna()\n",
    "    # Train (Year 1) / Test (Year 2)\n",
    "    split_idx = 252\n",
    "    train_data = returns.iloc[:split_idx]\n",
    "    test_data = returns.iloc[split_idx:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b06361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. DEEP LEARNING MODEL (Student-t AEM)\n",
    "\n",
    "# --- Math Utils ---\n",
    "def students_t_nll(x, ln_df, loc, ln_scale, dim=1):\n",
    "    \"\"\"Robust Student-t Negative Log Likelihood\"\"\"\n",
    "    nll = torch.lgamma((torch.exp(ln_df) + 1) / 2) - torch.lgamma(torch.exp(ln_df) / 2)\n",
    "    nll += (ln_scale - np.log(np.pi) - ln_df) / 2\n",
    "    nll -= (torch.exp(ln_df) + 1) * torch.log(1 + (torch.exp(ln_scale) * ((x - loc) ** 2) / torch.exp(ln_df))) / 2\n",
    "    return torch.sum(-1 * nll, dim=dim)\n",
    "\n",
    "def reparameterize(mu, ln_var):\n",
    "    std = torch.exp(0.5 * ln_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + std * eps\n",
    "\n",
    "def gaussian_kl(mu, ln_var, dim=1):\n",
    "    return torch.sum(-0.5 * (1 + ln_var - mu.pow(2) - ln_var.exp()), dim=dim)\n",
    "\n",
    "# --- Network ---\n",
    "class StudentsTVAE(nn.Module):\n",
    "    def __init__(self, n_in, n_latent=10, n_h=64):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(n_in, n_h), nn.Tanh(),\n",
    "            nn.Linear(n_h, n_latent * 2) # mu, ln_var\n",
    "        )\n",
    "        self.dec_h = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_h), nn.Tanh()\n",
    "        )\n",
    "        self.dec_params = nn.Linear(n_h, n_in * 3) # df, loc, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_enc = self.enc(x)\n",
    "        mu, ln_var = h_enc.chunk(2, dim=1)\n",
    "        z = reparameterize(mu, ln_var)\n",
    "        h_dec = self.dec_h(z)\n",
    "        params = self.dec_params(h_dec)\n",
    "        ln_df, loc, ln_scale = params.chunk(3, dim=1)\n",
    "        \n",
    "        RE = students_t_nll(x, ln_df, loc, ln_scale)\n",
    "        KL = gaussian_kl(mu, ln_var)\n",
    "        return RE + KL, loc, ln_scale\n",
    "\n",
    "# AEM Cleaner\n",
    "def clean_aem(returns, n_latent=10, epochs=1000):\n",
    "    \"\"\"\n",
    "    Advanced AEM: Uses 'Structured Trace Preservation'.\n",
    "    Keeps n_latent=10 (Smart) but uses the Model's learned noise profile \n",
    "    to create a safer, more accurate risk model.\n",
    "    \"\"\"\n",
    "    # 1. SCALING\n",
    "    scaler_mean = returns.mean(axis=0).values\n",
    "    scaler_std = returns.std(axis=0).values\n",
    "    scaler_std[scaler_std == 0] = 1.0\n",
    "    \n",
    "    scaled_returns = (returns - scaler_mean) / scaler_std\n",
    "    data = torch.FloatTensor(scaled_returns.values).to(device)\n",
    "    \n",
    "    # 2. MODEL SETUP\n",
    "    model = StudentsTVAE(n_in=returns.shape[1], n_latent=n_latent).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.002)\n",
    "    \n",
    "    # 3. TRAINING\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        loss_vector, _, _ = model(data)\n",
    "        loss = loss_vector.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    # 4. EXTRACTION\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, loc, ln_scale = model(data)\n",
    "        \n",
    "        # Signal (Location)\n",
    "        clean_scaled = loc.cpu().numpy()\n",
    "        \n",
    "        # Noise Shape (Student-t Scale)\n",
    "        # We take the average learned scale across the timeframe\n",
    "        avg_ln_scale = ln_scale.mean(dim=0).cpu().numpy()\n",
    "\n",
    "    # 5. RESCALING SIGNAL\n",
    "    clean_returns = (clean_scaled * scaler_std) + scaler_mean\n",
    "    clean_returns_df = pd.DataFrame(clean_returns, index=returns.index, columns=returns.columns)\n",
    "    \n",
    "    # 6. STRUCTURED TRACE PRESERVATION\n",
    "    \n",
    "    # A. Calculate Energies\n",
    "    cov_signal = clean_returns_df.cov()\n",
    "    trace_total = np.trace(returns.cov().values) # True total variance\n",
    "    trace_signal = np.trace(cov_signal.values)   # Variance captured by Autoencoder\n",
    "    missing_energy = max(trace_total - trace_signal, 1e-4) # Variance we need to put back\n",
    "    \n",
    "    # B. Calculate Learned Noise Shape\n",
    "    noise_shape = (np.exp(avg_ln_scale) * scaler_std) ** 2\n",
    "    \n",
    "    # C. Normalize Noise to fill the Missing Energy\n",
    "    scale_factor = missing_energy / np.sum(noise_shape)\n",
    "    final_noise_diag = noise_shape * scale_factor\n",
    "    \n",
    "    # 7. FINAL COVARIANCE\n",
    "    cov_final = cov_signal + np.diag(final_noise_diag)\n",
    "    \n",
    "    return cov_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a097e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. BASELINE CLEANERS\n",
    "def clean_rmt(returns, k_min=3):\n",
    "    T, N = returns.shape\n",
    "    cov = returns.cov().values\n",
    "    vols = np.sqrt(np.diag(cov))\n",
    "    D_inv = np.diag(1.0 / vols)\n",
    "    corr = D_inv @ cov @ D_inv\n",
    "    \n",
    "    evals, evecs = linalg.eigh(corr)\n",
    "    # Sort\n",
    "    idx = evals.argsort()[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    q = N / T\n",
    "    lambda_max = (1 + np.sqrt(q))**2\n",
    "    signal_mask = evals > lambda_max\n",
    "    signal_mask[:k_min] = True # Force market factors\n",
    "    \n",
    "    if (~signal_mask).sum() > 0:\n",
    "        evals[~signal_mask] = evals[~signal_mask].mean()\n",
    "        \n",
    "    corr_clean = evecs @ np.diag(evals) @ evecs.T\n",
    "    np.fill_diagonal(corr_clean, 1.0)\n",
    "    D = np.diag(vols)\n",
    "    return pd.DataFrame(D @ corr_clean @ D, index=returns.columns, columns=returns.columns)\n",
    "\n",
    "def clean_lw(returns):\n",
    "    lw = LedoitWolf()\n",
    "    lw.fit(returns)\n",
    "    return pd.DataFrame(lw.covariance_, index=returns.columns, columns=returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ca40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*****                 10%                       ]  2 of 20 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 20 Tech Stocks (2015-2017)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  20 of 20 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Matrices...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4. PORTFOLIO BACKTESTING\n",
    "def optimize_portfolio(cov_matrix, expected_returns):\n",
    "    num_assets = len(expected_returns)\n",
    "    \n",
    "    def neg_sharpe(weights):\n",
    "        ret = np.dot(weights, expected_returns)\n",
    "        vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        return -ret / vol\n",
    "\n",
    "    # Constraints: Sum weights = 1, Allow shorting [-1, 1] per paper logic\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((-1.0, 1.0) for _ in range(num_assets))\n",
    "    init_guess = num_assets * [1. / num_assets,]\n",
    "    \n",
    "    result = optimize.minimize(neg_sharpe, init_guess, method='SLSQP', \n",
    "                               bounds=bounds, constraints=constraints, tol=1e-10)\n",
    "    return result.x\n",
    "\n",
    "def backtest(weights, test_returns):\n",
    "    port_ret = test_returns.dot(weights)\n",
    "    ann_ret = port_ret.mean() * 252\n",
    "    ann_vol = port_ret.std() * np.sqrt(252)\n",
    "    sharpe = ann_ret / ann_vol\n",
    "    return ann_ret, ann_vol, sharpe, port_ret\n",
    "\n",
    "# 5. RUN THE EXPERIMENT\n",
    "train_df, test_df = get_real_data()\n",
    "print(\"\\nCleaning Matrices...\")\n",
    "cov_bench = train_df.cov()\n",
    "cov_rmt = clean_rmt(train_df)\n",
    "cov_lw = clean_lw(train_df)\n",
    "# Increase latent factors to 10 for better signal capture in 20 stocks\n",
    "cov_aem = clean_aem(train_df, n_latent=10, epochs=1000)\n",
    "print(\"Optimizing Portfolios...\")\n",
    "mu = train_df.mean()\n",
    "w_bench = optimize_portfolio(cov_bench.values, mu.values)\n",
    "w_rmt = optimize_portfolio(cov_rmt.values, mu.values)\n",
    "w_lw = optimize_portfolio(cov_lw.values, mu.values)\n",
    "w_aem = optimize_portfolio(cov_aem.values, mu.values)\n",
    "\n",
    "# Results\n",
    "r_bench = backtest(w_bench, test_df)\n",
    "r_rmt = backtest(w_rmt, test_df)\n",
    "r_lw = backtest(w_lw, test_df)\n",
    "r_aem = backtest(w_aem, test_df)\n",
    "\n",
    "print(f\"\\n{'Portfolio':<15} {'Return':<10} {'Vol':<10} {'Sharpe':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Benchmark':<15} {r_bench[0]:.2%}     {r_bench[1]:.2%}     {r_bench[2]:.4f}\")\n",
    "print(f\"{'RMT':<15} {r_rmt[0]:.2%}     {r_rmt[1]:.2%}     {r_rmt[2]:.4f}\")\n",
    "print(f\"{'LWS':<15} {r_lw[0]:.2%}     {r_lw[1]:.2%}     {r_lw[2]:.4f}\")\n",
    "print(f\"{'AEM (Ours)':<15} {r_aem[0]:.2%}     {r_aem[1]:.2%}     {r_aem[2]:.4f}\")\n",
    "\n",
    "# Diagnostics\n",
    "print(\"\\n--- Diagnostics ---\")\n",
    "print(f\"AEM Condition Number: {np.linalg.cond(cov_aem):.2f} (Target < 200)\")\n",
    "print(f\"RMT Condition Number: {np.linalg.cond(cov_rmt):.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "(1 + r_bench[3]).cumprod().plot(label='Benchmark', color='gray', linestyle='--')\n",
    "(1 + r_rmt[3]).cumprod().plot(label='RMT', color='green')\n",
    "(1 + r_aem[3]).cumprod().plot(label='AEM (Student-t)', color='red', linewidth=2)\n",
    "plt.title(\"Cumulative Wealth\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
